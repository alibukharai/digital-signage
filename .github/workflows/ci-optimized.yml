name: 🚀 CI/CD Pipeline (Optimized)

# Comprehensive CI/CD pipeline with modern best practices
# Optimized for performance, security, and maintainability

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
      - 'LICENSE'
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
      - 'LICENSE'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope to run'
        required: true
        default: 'standard'
        type: choice
        options:
          - quick      # Fast tests only (~5 min)
          - standard   # Standard test suite (~15 min)
          - full       # Complete test suite (~45 min)
      run_security_scan:
        description: 'Run security scanning'
        required: false
        default: true
        type: boolean

# Environment variables
env:
  PYTHON_VERSION: '3.9'
  CACHE_VERSION: 'v1'  # Increment to bust cache
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_NO_CACHE_DIR: 1

# Cancel previous runs on new commits
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ============================================================================
  # QUICK VALIDATION (Always runs - ~3-5 minutes)
  # ============================================================================
  quick-validation:
    name: 🔍 Quick Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      python-cache-key: ${{ steps.cache-key.outputs.python-cache-key }}
      should-run-tests: ${{ steps.changes.outputs.code-changed }}
      matrix: ${{ steps.matrix.outputs.matrix }}
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better change detection

    - name: 🔍 Detect Changes
      id: changes
      run: |
        # Check if code files changed (skip docs-only changes)
        if git diff --name-only ${{ github.event.before || 'HEAD~1' }} HEAD | grep -E '\.(py|yml|yaml|toml|cfg|ini|txt)$'; then
          echo "code-changed=true" >> $GITHUB_OUTPUT
          echo "📝 Code changes detected - will run tests"
        else
          echo "code-changed=false" >> $GITHUB_OUTPUT
          echo "📄 Only documentation changes - skipping tests"
        fi

    - name: 🐍 Setup Python
      if: steps.changes.outputs.code-changed == 'true'
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: 📦 Generate Cache Key
      if: steps.changes.outputs.code-changed == 'true'
      id: cache-key
      run: |
        # Create cache key based on Python version, OS, and dependencies
        CACHE_KEY="${{ env.CACHE_VERSION }}-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-$(sha256sum pyproject.toml setup.py 2>/dev/null | sha256sum | cut -d' ' -f1)"
        echo "python-cache-key=$CACHE_KEY" >> $GITHUB_OUTPUT
        echo "Generated cache key: $CACHE_KEY"

    - name: 🗂️ Cache Python Dependencies
      if: steps.changes.outputs.code-changed == 'true'
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.local/lib/python*/site-packages
          ./.venv
        key: ${{ steps.cache-key.outputs.python-cache-key }}
        restore-keys: |
          ${{ env.CACHE_VERSION }}-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

    - name: 📦 Install Dependencies
      if: steps.changes.outputs.code-changed == 'true'
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -e ".[dev]"

    - name: 🎨 Code Quality Checks
      if: steps.changes.outputs.code-changed == 'true'
      run: |
        echo "🎨 Running code quality checks..."
        
        # Black - Code formatting
        echo "📋 Checking code formatting with Black..."
        black --check --diff src/ tests/ || {
          echo "❌ Code formatting issues found"
          echo "::error::Run 'black src/ tests/' to fix formatting issues"
          exit 1
        }
        
        # isort - Import sorting
        echo "📋 Checking import sorting with isort..."
        isort --check-only --diff src/ tests/ || {
          echo "❌ Import sorting issues found"
          echo "::error::Run 'isort src/ tests/' to fix import sorting"
          exit 1
        }
        
        # flake8 - Linting
        echo "📋 Running flake8 linting..."
        flake8 src/ tests/ \
          --max-line-length=88 \
          --extend-ignore=E203,W503,E501 \
          --per-file-ignores="__init__.py:F401" \
          --statistics || {
          echo "❌ Linting issues found"
          echo "::error::Fix linting issues reported above"
          exit 1
        }

    - name: 🔍 Type Checking
      if: steps.changes.outputs.code-changed == 'true'
      run: |
        echo "🔍 Running type checking with mypy..."
        mypy src/ --ignore-missing-imports --no-strict-optional || {
          echo "⚠️ Type checking issues found (non-blocking)"
        }

    - name: 📋 Test Matrix Generation
      id: matrix
      if: steps.changes.outputs.code-changed == 'true'
      run: |
        # Generate test matrix based on available test categories
        if [ "${{ github.event.inputs.test_scope }}" = "full" ]; then
          MATRIX='["unit", "integration", "security", "performance"]'
        elif [ "${{ github.event.inputs.test_scope }}" = "standard" ]; then
          MATRIX='["unit", "integration"]'
        else
          MATRIX='["unit"]'
        fi
        echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
        echo "Generated test matrix: $MATRIX"

  # ============================================================================
  # UNIT TESTS (Parallel execution)
  # ============================================================================
  unit-tests:
    name: 🧪 Unit Tests
    runs-on: ubuntu-latest
    needs: quick-validation
    if: needs.quick-validation.outputs.should-run-tests == 'true'
    timeout-minutes: 15
    
    strategy:
      fail-fast: false
      matrix:
        test-group: ['core', 'infrastructure', 'application', 'domain']
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4

    - name: 🐍 Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: 🗂️ Restore Cache
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.local/lib/python*/site-packages
          ./.venv
        key: ${{ needs.quick-validation.outputs.python-cache-key }}
        restore-keys: |
          ${{ env.CACHE_VERSION }}-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: 🧪 Run Unit Tests
      run: |
        echo "🧪 Running unit tests for ${{ matrix.test-group }}..."
        python -m pytest tests/ \
          -m "not hardware and not integration and not slow" \
          --cov=src \
          --cov-report=xml:coverage-${{ matrix.test-group }}.xml \
          --cov-report=term-missing \
          --cov-report=html:htmlcov-${{ matrix.test-group }} \
          --junit-xml=junit-${{ matrix.test-group }}.xml \
          --tb=short \
          --durations=10 \
          -v \
          tests/*${{ matrix.test-group }}* || echo "Some tests in ${{ matrix.test-group }} failed"

    - name: 📊 Upload Coverage Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-${{ matrix.test-group }}
        path: |
          coverage-${{ matrix.test-group }}.xml
          htmlcov-${{ matrix.test-group }}/
          junit-${{ matrix.test-group }}.xml
        retention-days: 7

  # ============================================================================
  # SECURITY SCANNING
  # ============================================================================
  security-scan:
    name: 🔒 Security Scan
    runs-on: ubuntu-latest
    needs: quick-validation
    if: |
      needs.quick-validation.outputs.should-run-tests == 'true' && 
      (github.event.inputs.run_security_scan != 'false')
    timeout-minutes: 10
    
    permissions:
      security-events: write
      contents: read
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4

    - name: 🐍 Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: 🗂️ Restore Cache
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.local/lib/python*/site-packages
          ./.venv
        key: ${{ needs.quick-validation.outputs.python-cache-key }}

    - name: 📦 Install Security Tools
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install bandit[toml] safety semgrep pip-audit

    - name: 🔍 Bandit Security Linting
      run: |
        echo "🔍 Running Bandit security analysis..."
        bandit -r src/ \
          -f json \
          -o bandit-report.json \
          --severity-level medium \
          --confidence-level medium || true
        
        # Display critical findings
        if [ -f bandit-report.json ]; then
          echo "🚨 Security Issues Found:"
          jq -r '.results[]? | "⚠️ \(.filename):\(.line_number) - \(.test_name): \(.issue_text)"' bandit-report.json || echo "No issues found"
        fi

    - name: 🔒 Dependency Vulnerability Scan
      run: |
        echo "🔒 Scanning dependencies for vulnerabilities..."
        
        # Safety check
        safety check \
          --json \
          --output safety-report.json || true
        
        # pip-audit check
        pip-audit \
          --format=json \
          --output=pip-audit-report.json || true
        
        # Display findings
        echo "🚨 Dependency Vulnerabilities:"
        if [ -f safety-report.json ]; then
          jq -r '.vulnerabilities[]? | "⚠️ \(.package_name) v\(.installed_version) - \(.advisory)"' safety-report.json || echo "No vulnerabilities found"
        fi

    - name: 🔍 Semgrep Static Analysis
      run: |
        echo "🔍 Running Semgrep static analysis..."
        semgrep \
          --config=auto \
          --json \
          --output=semgrep-report.json \
          src/ || true
        
        # Display findings
        if [ -f semgrep-report.json ]; then
          echo "🚨 Static Analysis Findings:"
          jq -r '.results[]? | "⚠️ \(.path):\(.start.line) - \(.check_id): \(.extra.message)"' semgrep-report.json || echo "No issues found"
        fi

    - name: 📊 Upload Security Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          *-report.json
        retention-days: 30

    - name: 📋 Security Summary
      if: always()
      run: |
        echo "## 🔒 Security Scan Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        BANDIT_COUNT=$(jq '.results | length' bandit-report.json 2>/dev/null || echo "0")
        SAFETY_COUNT=$(jq '.vulnerabilities | length' safety-report.json 2>/dev/null || echo "0")
        SEMGREP_COUNT=$(jq '.results | length' semgrep-report.json 2>/dev/null || echo "0")
        
        echo "| Tool | Issues Found |" >> $GITHUB_STEP_SUMMARY
        echo "|------|-------------|" >> $GITHUB_STEP_SUMMARY
        echo "| Bandit | $BANDIT_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "| Safety | $SAFETY_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "| Semgrep | $SEMGREP_COUNT |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "$BANDIT_COUNT" -eq 0 ] && [ "$SAFETY_COUNT" -eq 0 ] && [ "$SEMGREP_COUNT" -eq 0 ]; then
          echo "✅ **Status:** No security issues detected" >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ **Status:** Security issues found - review recommended" >> $GITHUB_STEP_SUMMARY
        fi

  # ============================================================================
  # INTEGRATION TESTS (Hardware simulation)
  # ============================================================================
  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ubuntu-latest
    needs: [quick-validation, unit-tests]
    if: |
      needs.quick-validation.outputs.should-run-tests == 'true' &&
      (github.event.inputs.test_scope == 'standard' || github.event.inputs.test_scope == 'full')
    timeout-minutes: 20
    
    services:
      # Mock hardware services for integration testing
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4

    - name: 🐍 Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: 🗂️ Restore Cache
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.local/lib/python*/site-packages
          ./.venv
        key: ${{ needs.quick-validation.outputs.python-cache-key }}

    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: 🔧 Setup Mock Hardware Environment
      run: |
        echo "🔧 Setting up mock hardware environment..."
        
        # Install additional test dependencies
        sudo apt-get update
        sudo apt-get install -y bluetooth bluez-tools
        
        # Setup mock GPIO
        export MOCK_HARDWARE=true
        export REDIS_URL=redis://localhost:6379

    - name: 🧪 Run Integration Tests
      run: |
        echo "🧪 Running integration tests..."
        python -m pytest tests/ \
          -m "integration and not hardware" \
          --cov=src \
          --cov-report=xml:coverage-integration.xml \
          --cov-report=term-missing \
          --junit-xml=junit-integration.xml \
          --tb=short \
          --durations=10 \
          -v

    - name: 📊 Upload Integration Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          coverage-integration.xml
          junit-integration.xml
        retention-days: 7

  # ============================================================================
  # COVERAGE CONSOLIDATION
  # ============================================================================
  coverage-report:
    name: 📊 Coverage Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always() && needs.quick-validation.outputs.should-run-tests == 'true'
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4

    - name: 🐍 Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: 📦 Install Coverage Tools
      run: |
        pip install coverage[toml] codecov

    - name: 📥 Download Coverage Reports
      uses: actions/download-artifact@v4
      with:
        pattern: coverage-*
        merge-multiple: true

    - name: 📊 Combine Coverage Reports
      run: |
        echo "📊 Combining coverage reports..."
        coverage combine coverage-*.xml || echo "No coverage files to combine"
        coverage report --show-missing
        coverage xml -o combined-coverage.xml
        coverage html -d combined-htmlcov

    - name: 📤 Upload to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./combined-coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: 📊 Coverage Summary
      run: |
        echo "## 📊 Test Coverage Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        coverage report >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # HARDWARE TESTS (Ubuntu runner with conditional self-hosted support)
  # ============================================================================
  hardware-tests:
    name: 🔧 Hardware Tests
    runs-on: ubuntu-latest
    # Note: Changed from [self-hosted, rock-pi-3399] to ubuntu-latest for better availability
    # Hardware-specific tests will be skipped on ubuntu-latest runners
    # If self-hosted runners are available, update this back to: [self-hosted, rock-pi-3399]
    needs: [quick-validation, unit-tests]
    if: |
      needs.quick-validation.outputs.should-run-tests == 'true' &&
      (github.ref == 'refs/heads/main' || github.event.inputs.test_scope == 'full')
    timeout-minutes: 30
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4

    - name: 🔧 Hardware Environment Check
      run: |
        echo "🔧 Checking hardware environment..."
        echo "Hardware: $(cat /proc/device-tree/model 2>/dev/null || echo 'Unknown')"
        echo "Memory: $(free -h | grep Mem | awk '{print $3 "/" $2}')"
        echo "Python: $(python3 --version)"
        echo "Bluetooth: $(bluetoothctl --version 2>/dev/null || echo 'Not available')"

    - name: 🐍 Setup Environment
      run: |
        python3 -m pip install --upgrade pip
        pip3 install -e ".[dev]"

    - name: 🧪 Critical Hardware Tests
      run: |
        echo "🧪 Running critical hardware tests..."
        timeout 25m python3 run_tests.py --critical --hardware --verbose || {
          echo "⚠️ Some hardware tests failed"
          exit 1
        }

    - name: 🧹 Cleanup
      if: always()
      run: |
        # Clean up test artifacts
        rm -f /tmp/rockpi_test_*.json
        rm -f /tmp/test_*.log

  # ============================================================================
  # FINAL VALIDATION
  # ============================================================================
  final-validation:
    name: ✅ Final Validation
    runs-on: ubuntu-latest
    needs: [quick-validation, unit-tests, security-scan, integration-tests, coverage-report]
    if: always() && needs.quick-validation.outputs.should-run-tests == 'true'
    
    steps:
    - name: 📊 Workflow Summary
      run: |
        echo "## 🚀 CI/CD Pipeline Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Quick Validation | ${{ needs.quick-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Scan | ${{ needs.security-scan.result == 'success' && '✅ Passed' || needs.security-scan.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '✅ Passed' || needs.integration-tests.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Coverage Report | ${{ needs.coverage-report.result == 'success' && '✅ Generated' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Overall status
        if [[ "${{ needs.quick-validation.result }}" == "success" && 
              "${{ needs.unit-tests.result }}" == "success" && 
              ("${{ needs.security-scan.result }}" == "success" || "${{ needs.security-scan.result }}" == "skipped") ]]; then
          echo "🎉 **Overall Status:** PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Overall Status:** FAILED" >> $GITHUB_STEP_SUMMARY
        fi

    - name: ✅ Mark Success
      if: |
        needs.quick-validation.result == 'success' &&
        needs.unit-tests.result == 'success' &&
        (needs.security-scan.result == 'success' || needs.security-scan.result == 'skipped')
      run: |
        echo "✅ All critical checks passed!"
        echo "Ready for merge or deployment."

    - name: ❌ Mark Failure
      if: |
        needs.quick-validation.result == 'failure' ||
        needs.unit-tests.result == 'failure'
      run: |
        echo "❌ Critical checks failed!"
        echo "Please review and fix issues before proceeding."
        exit 1